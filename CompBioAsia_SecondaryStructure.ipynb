{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CompBioAsia_SecondaryStructure.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMGIOaDhnM9SZZYoZ3O1XBe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanielOlson/CompBioAsia/blob/main/CompBioAsia_SecondaryStructure.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting secondary structure\n",
        "\n",
        "This is our final project: We are going to build a model that can predict secondary structure of protein sequences. "
      ],
      "metadata": {
        "id": "S6WRWICRT-Ry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup\n",
        "Standard stuff - run each cell in this section once."
      ],
      "metadata": {
        "id": "kZpcBFNiUrFg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WrEltBEFxpA"
      },
      "outputs": [],
      "source": [
        "!wget http://www.princeton.edu/~jzthree/datasets/ICML2014/cullpdb+profile_5926_filtered.npy.gz\n",
        "!mv cullpdb+profile_5926_filtered.npy.gz data.npy.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rmgkpNueGMHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "XvDN8k4NGx_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.load('data.npy.gz')\n",
        "\n",
        "x = x.reshape(-1, 700, 57)\n",
        "aminos = torch.transpose(torch.tensor(x[:,:,0:22]), -1, -2).float()\n",
        "labels = torch.transpose(torch.tensor(x[:,:,22:31]), -1, -2).float()\n",
        "\n",
        "aminos_train = aminos[:5000]\n",
        "aminos_test = aminos[5000:]\n",
        "\n",
        "labels_train = labels[:5000]\n",
        "labels_test = labels[5000:]\n",
        "\n",
        "print(aminos.shape, labels.shape)\n",
        "print(aminos_train.shape, aminos_test.shape)\n",
        "print(labels_train.shape, labels_test.shape)\n",
        "\n",
        "\n",
        "alphabet = ['A', 'C', 'E', 'D', 'G', 'F', 'I', 'H', 'K', 'M', 'L', 'N', 'Q', 'P', 'S', 'R', 'T', 'W', 'V', 'Y', 'X','NoSeq']\n",
        "alpha_to_num = {c:i for i, c in enumerate(alphabet)}\n",
        "secondary_labels = ['L', 'B', 'E', 'G', 'I', 'H', 'S', 'T','NoSeq']\n",
        "label_to_num = {c:i for i, c in enumerate(secondary_labels)}\n"
      ],
      "metadata": {
        "id": "mZZ5lNAaHUAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_with_data(x, y,\n",
        "                    model, batch_size, \n",
        "                    steps, learning_rate, \n",
        "                    loss_function, checkin=100):\n",
        "  dev = 'cpu'\n",
        "  if torch.cuda.is_available():\n",
        "    dev = 'cuda:0'\n",
        "\n",
        "  model.to(dev)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  for step in range(steps):\n",
        "    batch_i = torch.randint(0, len(x), (batch_size,))\n",
        "    batch_x = x[batch_i].to(dev)\n",
        "    batch_y = y[batch_i].to(dev)\n",
        "\n",
        "    out = model(batch_x)\n",
        "\n",
        "    loss = loss_function(out, batch_y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if step % checkin == 0:\n",
        "      print('progress: {:.2%} '.format(step / steps), 'loss:', float(loss))\n",
        "\n",
        "  model.to('cpu')\n",
        "\n",
        "# This function just tests our accuracy\n",
        "def test_accuracy(x, y, model, threshold = 0.5, batch_size=32):\n",
        "  dev = 'cpu'\n",
        "  if torch.cuda.is_available():\n",
        "    dev = 'cuda:0'\n",
        "\n",
        "  model.to(dev)\n",
        "\n",
        "  correct_aas = 0\n",
        "  incorrect_aas = 0\n",
        "\n",
        "  correct_ss = 0\n",
        "  incorrect_ss = 0\n",
        "  with torch.no_grad():\n",
        "   # print(x.shape)\n",
        "    batches = int(len(x) / batch_size) - 1# we lose a little bit of data but thats ok\n",
        "   # print(batches)\n",
        "\n",
        "    for batch in range(batches):\n",
        "      start_idx = batch * batch_size\n",
        "\n",
        "      batch_x = x[start_idx:start_idx+batch_size].to(dev)\n",
        "      batch_y = y[start_idx:start_idx+batch_size].to(dev)\n",
        "\n",
        "      out = model(batch_x)\n",
        "      out = out > threshold\n",
        "\n",
        "      correct = out == batch_y\n",
        "      incorrect = out != batch_y\n",
        "\n",
        "      \n",
        "\n",
        "      noseq = batch_y[:,-1,:] == 0\n",
        "      not_interesting = batch_y[:,:-1,:] \n",
        "      not_interesting = not_interesting > 0.5\n",
        "      not_interesting = torch.sum(not_interesting, dim=1).view(-1, 1, 700)\n",
        "      not_interesting = not_interesting > 0\n",
        "      not_interesting = not_interesting.view(-1, 1, 700)\n",
        "  #    print(not_interesting.shape)\n",
        "      noseq = noseq.unsqueeze(1)\n",
        "      correct *= noseq\n",
        "      incorrect *= noseq\n",
        "\n",
        "      correct_aas += torch.sum(correct)\n",
        "      incorrect_aas += torch.sum(incorrect)\n",
        "\n",
        "      correct *= not_interesting\n",
        "      incorrect *= not_interesting\n",
        "\n",
        "      correct_ss += torch.sum(correct)\n",
        "      incorrect_ss += torch.sum(incorrect)\n",
        "\n",
        "   #   comparison = torch.where(batch_y[:,8,:])\n",
        "\n",
        "     \n",
        "\n",
        "  model.to('cpu')\n",
        "  return float(correct_aas / (correct_aas + incorrect_aas)), float(correct_ss / (correct_ss + incorrect_ss))\n"
      ],
      "metadata": {
        "id": "yOPZw4K2M1GQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Build, train, and test our model\n",
        "\n",
        "1.   Making the network deeper (more layers).\n",
        "2.   Making the network wider (more neurons).\n",
        "3.   Try bigger and smaller batch sizes.\n",
        "4.   Try more/less steps.\n",
        "5.   Using different activation functions https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity. \n",
        "6.   Try using different loss functions"
      ],
      "metadata": {
        "id": "T-O__j5hUnF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32 # How many images we train on at every step\n",
        "steps = 2000 # How many total steps we will train for\n",
        "learning_rate = 0.0001 # How fast we adjust gradients with gradient descent\n",
        "loss_function = nn.MSELoss() # Which loss function we're using\n",
        "checkin = int(steps / 5) # How often we print our loss (smaller = more frequently)\n",
        "\n",
        "\n",
        "# Here's our starting place.\n",
        "# To add more Conv1d layers you'll want to use:\n",
        "# nn.Conv1d(input_channels, output_channels, kernel_size=your_kernel_size, stride=1, padding='same')\n",
        "# You can replace input_channels, output_channels, and kernel_size with whatever you want\n",
        "\n",
        "model = nn.Sequential(nn.Conv1d(22, 10, kernel_size=1, stride=1, padding='same'),\n",
        "                      nn.ELU(),\n",
        "                      nn.Conv1d(10, 9, kernel_size=5, stride=1, padding='same'),\n",
        "                      nn.Sigmoid())\n",
        "\n",
        "print(test_accuracy(aminos_test, labels_test, model))\n",
        "\n",
        "\n",
        "\n",
        "train_with_data(aminos_train, labels_train, model, \n",
        "                batch_size, steps, learning_rate, \n",
        "                loss_function, checkin=checkin)\n",
        "\n",
        "\n",
        "print(test_accuracy(aminos_test, labels_test, model))"
      ],
      "metadata": {
        "id": "H7LLKk2RMjOF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}