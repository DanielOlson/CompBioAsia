{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CompBioAsia_nn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMXXtf7oZZxQdlQaL31z1tw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanielOlson/CompBioAsia/blob/main/CompBioAsia_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# REAL Deep Learning\n",
        "We are now ready to do real deep learning. Here you're going to build your own models to try to solve real problems. I'll take care of the optimization code so you can focus on the fun stuff. Your goal is to get the largest testing accuracy you can!"
      ],
      "metadata": {
        "id": "UoSUF48Hnn0P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST\n",
        "The first thing we're going to try to learn is hand written digits. First we'll download the MNIST dataset (our dataset of hand written digits). Then we'll build a a model and train that model on MNIST. Finally we'll test our model and try to figure out how we can make our model better."
      ],
      "metadata": {
        "id": "ge96U4_mC0X4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup\n",
        "\n",
        "---\n",
        "\n",
        "Load libraries, download data, build some helper functions"
      ],
      "metadata": {
        "id": "AR0V_g-_3lYk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xt6XtJQaAjN0"
      },
      "outputs": [],
      "source": [
        "import torch # The big pytorch library\n",
        "from torch import nn # Pytorch's neural network library\n",
        "import torch.nn.functional as F # Some useful helper functions\n",
        "\n",
        "# Torchvision will let us load up the MNIST dataset EZ PZ\n",
        "from torchvision import datasets, transforms \n",
        "\n",
        "import matplotlib.pyplot as plt # Matplotlib for visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell grabs MNIST and loads it into tensors for us\n",
        "\n",
        "transform = transforms.Compose([\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(0.1307, 0.3081)\n",
        "])\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_data = datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transform)\n",
        "test_data = datasets.MNIST('../data', train=False, download=True,\n",
        "                       transform=transform)\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=len(train_data), shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=len(test_data), shuffle=True)\n",
        "\n",
        "x_train, y_train = iter(train_loader).next()\n",
        "x_train = x_train.squeeze()\n",
        "y_train = F.one_hot(y_train)\n",
        "y_train = y_train.float()\n",
        "\n",
        "x_small = x_train[0:500]\n",
        "y_small = y_train[0:500]\n",
        "#y_small = F.one_hot(y_small)\n",
        "y_small = y_small.float()\n",
        "\n",
        "x_test, y_test = iter(test_loader).next()\n",
        "x_test = x_test.squeeze()\n",
        "y_test = F.one_hot(y_test)\n",
        "y_test = y_test.float()\n",
        "\n",
        "\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "\n",
        "print(\"x_test shape:\", x_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ],
      "metadata": {
        "id": "h3CxFB-yB1ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell has a bunch of helper functions\n",
        "\n",
        "def train_with_data(x, y, \n",
        "                    model, batch_size, \n",
        "                    steps, learning_rate, \n",
        "                    loss_function, checkin=100):\n",
        "  dev = 'cpu'\n",
        "  if torch.cuda.is_available():\n",
        "    dev = 'cuda:0'\n",
        "\n",
        "  model.to(dev)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  for step in range(steps):\n",
        "    batch_i = torch.randint(0, len(x), (batch_size,))\n",
        "    batch_x = x[batch_i].to(dev)\n",
        "    batch_y = y[batch_i].to(dev)\n",
        "\n",
        "    out = model(batch_x)\n",
        "    loss = loss_function(out, batch_y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if step % checkin == 0:\n",
        "      print('progress: {:.2%} '.format(step / steps), 'loss:', float(loss))\n",
        "\n",
        "  model.to('cpu')\n",
        "\n",
        "# This function just tests our accuracy\n",
        "def test_accuracy(x, y, model, show_failures=0):\n",
        "  dev = 'cpu'\n",
        "  if torch.cuda.is_available():\n",
        "    dev = 'cuda:0'\n",
        "\n",
        "  model.to(dev)\n",
        "  x = x.to(dev)\n",
        "  y = y.to(dev)\n",
        "\n",
        "  result = 0\n",
        "  with torch.no_grad():\n",
        "    out = model(x)\n",
        "\n",
        "    y = torch.argmax(y, dim=-1)\n",
        "    out = torch.argmax(out, dim=-1)\n",
        "    \n",
        "    result = out == y\n",
        "    result = result.float()\n",
        "    result = torch.mean(result, dim=-1)\n",
        "\n",
        "    if show_failures > 0:\n",
        "      failures = out != y\n",
        "\n",
        "      failure_out = out[failures]\n",
        "      failure_y = y[failures]\n",
        "\n",
        "      failure_images = x[failures]\n",
        "      \n",
        "      random_failures = torch.randint(0, len(failure_images), (show_failures,))\n",
        "\n",
        "      for f in random_failures:\n",
        "        print(\"Target:\", int(failure_y[f]), \"Predicted:\", int(failure_out[f]))\n",
        "        plt.imshow(failure_images[f].to('cpu'))\n",
        "        plt.show()\n",
        "\n",
        "  model.to('cpu')\n",
        "\n",
        "  return float(result)"
      ],
      "metadata": {
        "id": "Ts7oQNAh332S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Digits and recognizing them!\n",
        "\n",
        "---\n",
        "\n",
        "The interesting code starts here!"
      ],
      "metadata": {
        "id": "zDWpVGGn3jZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's have a look at some of these hand written digits!\n",
        "\n",
        "figure = plt.figure()\n",
        "num_of_images = 100\n",
        "for index in range(1, num_of_images + 1):\n",
        "    plt.subplot(10, 10, index)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(x_train[index], cmap='gray_r')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Gf4W15ujBerO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training on the entire trainset\n",
        "\n",
        "---\n",
        "\n",
        "It's finally time! The place holder model I've made is awful. Real bad. It connects 784 inputs to a single neuron, and then connects that neuron to 10 outputs. The first layer of the model needs to stay there (nn.Flatten()), and the last layer of the model should stay there (nn.Softmax()). Everything else can change! Some ways you might try to make improvements:\n",
        "\n",
        "\n",
        "\n",
        "1.   Making the network deeper (more layers).\n",
        "2.   Making the network wider (more neurons).\n",
        "3.   Using different activation functions (replacing nn.Tanh) such as nn.Sigmoid(), nn.ReLU(), nn.ELU(). a complete list of activation functions can be found at https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity. \n",
        "4.   Try bigger and smaller batch sizes\n",
        "5.   Try more/less steps\n",
        "\n",
        "Feel free to change any of the code in the cell, and also feel free to use any of the tools in the nn library: https://pytorch.org/docs/stable/nn.html"
      ],
      "metadata": {
        "id": "1E_MkWgSE8kD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "batch_size = 32 # How many images we train on at every step\n",
        "steps = 20000 # How many total steps we will train for\n",
        "learning_rate = 0.001 # How fast we adjust gradients with gradient descent\n",
        "loss_function = nn.MSELoss() # Which loss function we're using\n",
        "checkin = int(steps / 5) # How often we print our loss (smaller = more frequently)\n",
        "\n",
        "# Change this model to change try and improve results\n",
        "model = nn.Sequential(nn.Flatten(), # <--- the model needs to begin with this\n",
        "                      nn.Linear(28*28, 1), \n",
        "                      nn.Tanh(),\n",
        "                      nn.Linear(1, 10), \n",
        "                      nn.Softmax(dim=-1)) # <--- the model needs to end with this\n",
        "\n",
        "# Here we get a starting testing accuracy\n",
        "acc = test_accuracy(x_test, y_test, model)\n",
        "print(\"Starting accuracy: {:.2%}\".format(acc))\n",
        "\n",
        "# Train our model with our train_with_data helper function\n",
        "train_with_data(x_train, y_train,\n",
        "                model, batch_size, steps,\n",
        "                learning_rate, loss_function, checkin=checkin)\n",
        "\n",
        "# Check the ending testing accuracy\n",
        "acc = test_accuracy(x_test, y_test, model)\n",
        "print(\"Ending accuracy: {:.2%}\".format(acc))\n",
        "\n",
        "# Show some random failed predictions\n",
        "_ = test_accuracy(x_test, y_test, model, show_failures=1) \n",
        "# Change this to change how many failures are shown    ^"
      ],
      "metadata": {
        "id": "TiIbThnJ76p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training on a small trainset\n",
        "\n",
        "---\n",
        "\n",
        "Before we were training on a 'uuuge dataset. Well maybe not ginormous, it was a whole 60,000 images - the entire MNIST training set.\n",
        "\n",
        "Deep learning becomes way harder when we have a smaller dataset to work with. Below we're going to try to build a model that can learn using only 500 images. Feel free to change naything in the cell below"
      ],
      "metadata": {
        "id": "gZrBcJGWHr4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "batch_size = 32 # How many images we train on at every step\n",
        "steps = 20000 # How many total steps we will train for\n",
        "learning_rate = 0.001 # How fast we adjust gradients with gradient descent\n",
        "loss_function = nn.MSELoss() # Which loss function we're using\n",
        "checkin = int(steps / 5) # How often we print our loss (smaller = more frequently)\n",
        "\n",
        "# Change this model to change try and improve results\n",
        "model = nn.Sequential(nn.Flatten(), # <--- the model needs to begin with this\n",
        "                      nn.Linear(28*28, 1), \n",
        "                      nn.Tanh(),\n",
        "                      nn.Linear(1, 10), \n",
        "                      nn.Softmax(dim=-1)) # <--- the model needs to end with this\n",
        "\n",
        "# Here we get a starting testing accuracy\n",
        "acc = test_accuracy(x_test, y_test, model)\n",
        "print(\"Starting accuracy: {:.2%}\".format(acc))\n",
        "\n",
        "# Train our model with our train_with_data helper function\n",
        "train_with_data(x_small, y_small,\n",
        "                model, batch_size, steps,\n",
        "                learning_rate, loss_function, checkin=checkin)\n",
        "\n",
        "# Check the ending testing accuracy\n",
        "acc = test_accuracy(x_test, y_test, model)\n",
        "print(\"Ending accuracy: {:.2%}\".format(acc))\n",
        "\n",
        "# Show some random failed predictions\n",
        "_ = test_accuracy(x_test, y_test, model, show_failures=1) \n",
        "# Change this to change how many failures are shown    ^"
      ],
      "metadata": {
        "id": "IUEkzabcHrN1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}