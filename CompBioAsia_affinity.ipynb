{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CompBioAsia_affinity.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNHoWeIcb1lEx9aSVKMhVXU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanielOlson/CompBioAsia/blob/main/CompBioAsia_affinity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Binding Affinity Prediction!\n",
        "We're now dangerous and ready to do REAL machine learning on REAL problems. Good luck."
      ],
      "metadata": {
        "id": "2OTJnjplCzvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup\n",
        "Normal setup cells, run each one once. NOTE: the URL should not be \"2hm\" but instead \"2mh\". Switch the m and h around to download the file."
      ],
      "metadata": {
        "id": "Y6Oz0GmKDmFA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIY0l3MjKd-m"
      },
      "outputs": [],
      "source": [
        "!wget https://umt.box.com/shared/static/2hmfpjcnprr1jt5rik45edghdy0rnw0p.zip\n",
        "!unzip *.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "4yL5_V6sK6Ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_all_data(indx_file, torch_file, cutoff=30.0, clean=False):\n",
        "  names = []\n",
        "  torch_data = torch.load(torch_file)\n",
        "  with open(indx_file, 'r') as file:\n",
        "    for line in file:\n",
        "      line = line.split(' ')\n",
        "      if len(line) > 0:\n",
        "        names.append((line[0], line[1]))\n",
        "  \n",
        "  ones = torch_data[:,-1] <= cutoff\n",
        "  zeros = torch_data[:,-1] > cutoff\n",
        "\n",
        "  torch_data[ones,-1] = 1.0\n",
        "  torch_data[zeros,-1] = 0.0\n",
        "\n",
        "  zero_field = torch.zeros_like(torch_data)\n",
        "  if clean:\n",
        "    for i in range(17):\n",
        "      zero_field[torch_data[:,i] == 0,i] = 1\n",
        "      torch_data[torch_data[:,i] != 0,i] = torch_data[torch_data[:,i] != 0,i] - torch.mean(torch_data[torch_data[:,i] != 0,i], dim=0)\n",
        "      torch_data[torch_data[:,i] != 0,i] = torch_data[torch_data[:,i] != 0,i] / torch.std(torch_data[torch_data[:,i] != 0,i], dim=0)\n",
        "  \n",
        "    torch_data = torch.cat([zero_field, torch_data], dim=-1)\n",
        "\n",
        "  return names, torch_data\n",
        "\n",
        "def train_with_data(x,\n",
        "                    model, batch_size, \n",
        "                    steps, learning_rate, \n",
        "                    loss_function, checkin=100):\n",
        "  dev = 'cpu'\n",
        "  if torch.cuda.is_available():\n",
        "    dev = 'cuda:0'\n",
        "\n",
        "  model.to(dev)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  if batch_size % 2 == 1:\n",
        "    print(\"Batch size must be divisible by 2\")\n",
        "    return -1\n",
        "\n",
        "  pos_x = x[x[:,-1] > 0.5]\n",
        "  neg_x = x[x[:,-1] < 0.5]\n",
        "\n",
        "  half_batch = int(batch_size / 2)\n",
        "\n",
        "  for step in range(steps):\n",
        "    pbatch_i = torch.randint(0, len(pos_x), (half_batch,))\n",
        "    nbatch_i = torch.randint(0, len(neg_x), (half_batch,))\n",
        "\n",
        "    batch_y = pos_x[pbatch_i,-1].view(-1, 1).to(dev)\n",
        "    batch_x = pos_x[pbatch_i,:-1].to(dev)\n",
        "\n",
        "    out = model(batch_x)\n",
        "    loss = loss_function(out, batch_y)\n",
        "\n",
        "    batch_y = neg_x[nbatch_i,-1].view(-1, 1).to(dev)\n",
        "    batch_x = neg_x[nbatch_i,:-1].to(dev)\n",
        "\n",
        "    out = model(batch_x)\n",
        "    loss += loss_function(out, batch_y)\n",
        "    \n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    if step % checkin == 0:\n",
        "      print('progress: {:.2%} '.format(step / steps), 'loss:', float(loss))\n",
        "\n",
        "  model.to('cpu')\n",
        "\n",
        "# This function just tests our accuracy\n",
        "def test_accuracy(x, model, batch_size=512):\n",
        "  dev = 'cpu'\n",
        "  if torch.cuda.is_available():\n",
        "    dev = 'cuda:0'\n",
        "\n",
        "  model.to(dev)\n",
        "\n",
        "  y = x[:,-1].view(-1, 1)\n",
        "  x = x[:,:-1]\n",
        "\n",
        "  x = x.to(dev)\n",
        "  y = y.to(dev)\n",
        "\n",
        "  pos_correct = 0\n",
        "  pos_incorrect = 0\n",
        "  neg_correct = 0\n",
        "  neg_incorrect = 0\n",
        "  with torch.no_grad():\n",
        "\n",
        "    batches = int(len(x) / batch_size) - 1# we lose a little bit of data but thats ok\n",
        "\n",
        "    for batch in range(batches):\n",
        "      start_idx = batch * batch_size\n",
        "\n",
        "      batch_x = x[start_idx:start_idx+batch_size]\n",
        "      batch_y = y[start_idx:start_idx+batch_size]\n",
        "\n",
        "      out = model(batch_x)\n",
        "      out = out > 0.5\n",
        "\n",
        "      pos_correct += torch.sum((out == batch_y)[batch_y > 0.5])\n",
        "      pos_incorrect += torch.sum((out != batch_y)[batch_y > 0.5])\n",
        "\n",
        "      neg_correct += torch.sum((out == batch_y)[batch_y < 0.5])\n",
        "      neg_incorrect += torch.sum((out != batch_y)[batch_y < 0.5])\n",
        "\n",
        "     \n",
        "\n",
        "  model.to('cpu')\n",
        "  return float(pos_correct / (pos_correct + pos_incorrect)), float(neg_correct / (neg_correct + neg_incorrect)), float((pos_correct + neg_correct) / (pos_correct + neg_correct + pos_incorrect + neg_incorrect))\n",
        "\n",
        "def pct_ones(data):\n",
        "  return torch.sum(data[:,-1] == 1.0) / len(data)\n",
        "\n",
        "\n",
        "dude_names, dude_data = read_all_data('./affinity_data/dude.indx', \n",
        "                                      './affinity_data/dude.torch')\n",
        "\n",
        "litpcba_names, litpcba_data = read_all_data('./affinity_data/litpcba.indx', \n",
        "                                          './affinity_data/litpcba.torch')\n",
        "\n",
        "\n",
        "print('DUD-E Shape:', dude_data.shape)\n",
        "print('Lit-PCBA Shape:', litpcba_data.shape)\n",
        "\n",
        "print('DUD-E percentage active: {:.4%}'.format(pct_ones(dude_data)))\n",
        "print('Lit-PCBA percentage active: {:.4%}'.format(pct_ones(litpcba_data)))"
      ],
      "metadata": {
        "id": "gfu9VwHiK9ZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ITS TIME !!\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "This problem is a lot harder than MNIST. Below is the code needed to train on the DUD-E dataset. It looks a lot like the MNIST code, but it will be more difficult to get traction.\n",
        "\n",
        "1.   Making the network deeper (more layers).\n",
        "2.   Making the network wider (more neurons).\n",
        "3.   Try bigger and smaller batch sizes.\n",
        "4.   Try more/less steps.\n",
        "5.   Using different activation functions (replacing nn.Tanh) such as nn.Sigmoid(), nn.ReLU(), nn.ELU(). a complete list of activation functions can be found at https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity. \n",
        "6.   Try using different loss functions (Maybe give nn.BCELoss() a go)"
      ],
      "metadata": {
        "id": "xeBMs5crEGL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "batch_size = 512 # How many images we train on at every step\n",
        "steps = 20000 # How many total steps we will train for\n",
        "learning_rate = 0.0001 # How fast we adjust gradients with gradient descent\n",
        "loss_function = nn.MSELoss() # Which loss function we're using\n",
        "checkin = int(steps / 5) # How often we print our loss (smaller = more frequently)\n",
        "\n",
        "# Change this model to change try and improve results\n",
        "model = nn.Sequential(nn.Linear(17, 5), \n",
        "                      nn.Tanh(),\n",
        "                      nn.Linear(5, 1),\n",
        "                      nn.Sigmoid()) # <--- the model needs to end with this\n",
        "\n",
        "# Here we get a starting testing accuracy\n",
        "# Accuracy is reported as (positive accuracy, negative accuracy, total accuracy)\n",
        "pos_acc, neg_acc, tot_acc = test_accuracy(dude_data, model)\n",
        "print(\"Starting DUD-E accuracy: ({:.4%}, {:.4%} = {:.4%})\".format(pos_acc, neg_acc, tot_acc))\n",
        "\n",
        "print(\"Training on DUD-E...\")\n",
        "# Train our model with our train_with_data helper function\n",
        "train_with_data(dude_data,\n",
        "                model, batch_size, steps,\n",
        "                learning_rate, loss_function, checkin=checkin)\n",
        "\n",
        "# Check the ending testing accuracy\n",
        "pos_acc, neg_acc, tot_acc = test_accuracy(dude_data, model)\n",
        "print(\"Ending DUD-E accuracy: ({:.4%}, {:.4%} {:.4%})\".format(pos_acc, neg_acc, tot_acc))\n"
      ],
      "metadata": {
        "id": "X_VFTdARL6xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test how we do on LitPCBA (we dont' do great)\n",
        "\n",
        "pos_acc, neg_acc, tot_acc = test_accuracy(litpcba_data, model)\n",
        "print(\"litPCBA: ({:.4%}, {:.4%} {:.4%})\".format(pos_acc, neg_acc, tot_acc))\n"
      ],
      "metadata": {
        "id": "XuW2DpDDBhEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show distributions of inputs\n",
        "\n",
        "r = torch.randint(0, len(dude_data), (10000,))\n",
        "x = torch.arange(len(r))\n",
        "\n",
        "for i in range(18):\n",
        "  print(i)\n",
        "  y, _ = torch.sort(dude_data[r, i])\n",
        "  plt.scatter(x, y)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "CxTx2K201ar6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}